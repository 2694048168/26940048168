<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[大数据框架Hadoop的前世今生]]></title>
    <url>%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6Hadoop%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%2F</url>
    <content type="text"><![CDATA[一纵两横一纵两横的思维&emsp;&emsp;即学习一个新学科时，可以看其一纵，其整个历史至今的发展过程。然后看其两横，一横是不同人对于其的评价即定义，还有一横是不同学科或领域与其的联系和区别。抓住这一纵两横的思维，可以帮助我们快速了解一个新的学科或者一个新的领域。&emsp;&emsp;在了解一件自己完全没有涉足过的事情之前，笔者喜欢使用“一纵两横”的思维去学习了解，这样能够快速的帮助自己构建知识体系。所谓的“一纵”，就是事物本身的历史发展，从最开始的诞生到当今的发展情况，可能有的人会觉得无聊或者没有必要，但笔者想告诉读者的是，这是进入一个事物领域的最基本素质，能够很好地体现着专业素养。而所谓的“两横”，就是一方面比较该领域里不同的专家学者或者提出者（创造者），以及其他领域的人对于该事物的阐述理解或者评价态度；另一方面就是比较已有领域学科或者相近领域学科和该事物的异同点或者联系与区别。但完成了这“一纵两横”后，不仅是从本质上了解事物，同时也和自己已有的知识架构联系在一起，有了自己的一些理解感悟。 大数据起源背景！&emsp;&emsp;笔者第一次正式接触大数据，是阅读维克托的《大数据时代》，当然是翻译的中文版，是浙江人民出版社的，在《大数据时代》书中提到，大数据的发展来源，大数据的特征，著者维克托传递出大数据时代来临了的信号，同时我们数据的思考和应用都应该有巨大的思想变革，这样才能适应大数据时代的发展。书中对大数据的三个核心思想： 数据不是随机样本，而是全部数据 数据不是精确性，而是混杂性 数据间不是因果关系，而是相关关系 &emsp;&emsp;维克托围绕着三个核心思想，阐述了我们思维、生活以及商业的变革，同时笔者有点小感悟，那就是我们的学习或者教育是不是也需要变革，针对这三个核心思想对我们学习或者教育进行改革，当然这不是我们重点，这就当做是笔者在抛砖引玉 (自恋一下 ̂_ ̂)。&emsp;&emsp;既然大数据这个概念能够催生，那么在实际的生活必然有着对应的实际应用。那么这又是这样的情况了？这就聊到了 hadoop 框架的产生背景了，进入 21 世纪以来，随着信息技术和信息社会的高速发展，信息或者数据在不断地增长，而且是超几何的疯狂增长。特别是在 Web2.0 时代，人们对于信息的产生和索取需求在疯狂增长，这样在浏览器端产生的数据在 TB 级别或者 PB 级别在增长，亿万网民用户的浏览数据记录以及日夜增长的网站，这些海量数据如何进行存储和分析计算，就成为摆在我们的面前，同时也是 Google 这样的搜索引擎公司天然就需要面对的现实和需要解决的问题。Google-Alphabet 的新老三篇文章，被称之为大数据领域的三驾马车： 2003 年发表文章 &lt;GFS:The Google File System&gt; 2004 年发表文章 &lt;MapReduce:Simlifed Data Processing on Large Clusters&gt; 2006 年发表文章 &lt;BigTable:A Distrbuted Stroage System for Structured Data&gt; 2010 年发表文章 &lt;Dremel: Interactive Analysis of Web-Scale Datasets&gt; 2010 年发表文章 &lt;Pregel: A System for Large-Scale Graph Processing&gt; Google 老三架马车： GFS、 MapReduce、 BigTable Google 新三架马车： Dremel、 Pregel、 Caffeine &emsp;&emsp;大数据的应用开发框架，你一定听过 Hadoop，对就是它！Hadoop 是 Apache(这样说应该不准确，就先这样理解) 基于 Google 的前三篇文章实现的开源框架，现在是 Apache 下一个顶级项目。大家可以访问官网看一看:Hadoop Apache官网 Hadoop 的起源背景之 GFS大数据解决本质问题之一，就是对海量的数据如何进行存储。&emsp;&emsp;海量的数据并不是传统的 MB 或者 GB 级数据，而是 TB、 PB 级的数据概念。或许你没有啥直观的感觉，我们用数据来进行表明一下： 8bit = 1Byte 210Byte = 1024Byte = 1KB 220KB = 1024KB = 1MB 230KB = 1024KB = 1GB 240GB = 1024GB = 1TB 250TB = 1024TB = 1PB 260PB = 1024PB = 1EB 270EB = 1024EB = 1ZB &emsp;&emsp;简单来说， 1.2ZB 字节数据，如果储存在只读光盘上，那么这些光盘可以堆成五堆，每一堆都可以伸到月球。公元前 3 世纪，埃及的托勒密二世竭力收集了当时所有的书写作品，全部储存在亚历山大图书馆，代表着当时世界上所有的知识量，但是在数字数据洪流涌向世界后，每一个人都可以获取大量的数据信心，相当与当时压力山大图书馆储存的数据总量的 320 倍。从上面的小故事就可以知道，现在数字信息的庞大，如此海量的数据需要储存，传统的磁盘阵列储存已经无法满足这样的需求了，毕竟磁盘阵列价格是昂贵的。这就需要低成本、高效率、高可靠的储存设计。2003 年， Google 发表了 文章，解决了这个问题。在文章中阐述了解决海量数据储存的设计思想。同时在 Apache 下Lucene 的子项目研究下，实现了海量数据的存储设计：分布式文件系统，也就是 HDFS（Hadoop Distributed File System）。 Hadoop 的起源背景之 MapReduce大数据解决本质问题之二，就是海量数据如何进行计算。&emsp;&emsp;在编程计算里，有并行编程计算框架，有过了解的人就知道，这并不是什么新兴的技术。同样 Google 在 2004 年发表了 &lt;MapReduce: Simplifed Data Processing on Large Clusters&gt; 文章，文章阐述了基于分布式储存的海量数据并行计算解决方案思想。开源社区 Apache 的 Hadoop 项目研究实现了MapReduce 并行计算框架，将计算与数据在本地进行，将数据分为 Map 和Reduce 阶段。简单阐述就是 MapReduce 编程模型：把一个大任务拆分成小任务，再进行汇总。 Hadoop 的起源背景之 BigTable大数据解决本质问题之三，就是对于海量的数据进行分析处理&emsp;&emsp;数据在储存后，其作用就是提供检索和查阅，这才是搜索引擎的功效，也是Google 的强大技术支持。那么提高查询和利用数据的效率就是需要解决的重点。到这里就需要有一定的数据库相关知识 (建议可以查阅一下关于数据库的起源以及历史发展)，数据库的产生就是为了查询和利用数据的效率提高，然而现有的数据库并不能满足基于分布式储存的需求。结构化的数据库 (SQL) 和非结构化的数据 (NoSQL)。&emsp;&emsp;Google 工程师在 2006 年发表了 &lt;Bigtable: A Distributed Storage System for Structured Data&gt; 文章，文中阐述了基于分布式储存的数据库设计思想。就这样数据库时代从关系型数据库进入了非关系型数据库时代，一张大表 BigTable 设计思想， BigTable 就是把所有的数据保存到一张表中，同时采用冗余方式 (提高效率和可靠性，这种冗余的方式是最常用的手段，无论是在通信领域，或者自然语言处理领域、语音处理等等), 基于其设计思想就开源实现了基于 HDFS 的非关系型数据库（NoSQL 数据库） HBase。&emsp;&emsp;小提示：其实在我们常用的云盘或者网盘，其主要的设计思想就是这样的，笔者常用的百度网盘也就类似于这样分布式的储存。笔者建议感兴趣的读者，可以阅读 Google 的三篇文章，最好是原文 (虽然笔者的英文很烂)，这样我们就更能理解其设计思想的精髓。 Hadoop 环境搭建Hadoop 分布式集群简介！&emsp;&emsp;引言：学习大数据，就需要自己搭建 Hadoop 的运行环境，这对于新手而言是一项困难的工作，特别是对于 Liunx 操作不熟悉的读者而言，更是一大难题。但是学习部署自己的 Hadoop 环境是学习大数据的必经之路，也是必会技能之一。接下来跟笔者一起来搭建 Hadoop 集群环境吧！当然在 Windows 下也是支持部署 hadoop 的，不过这并不适合实际生产的需求，同时 Windows 不如 Liunx 性能稳定等等因素，一次是来自 Apache 官网对于开发平台的简介。&emsp;&emsp;GNU/Linux is supported as a development and production platform. Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes.Windows is also a supported platform but the followings steps are for Linux only. To set up Hadoop on Windows, see wiki page.&emsp;&emsp;支持 GNU / Linux 作为开发和生产平台。已经在具有 2000 个节点的GNU / Linux 集群上演示了 Hadoop。 Windows 也是受支持的平台，但以下步骤仅适用于 Linux。要在 Windows 上设置 Hadoop，请参阅 Wiki 页面。 在学习大数据之前，我们应该知道对于 Hadoop 的环境搭建，有着三种方式： 本地模式，就是单机版的 Hadoop，笔者觉得完全没有必要，因为 Hadoop的储存原理本就是分布式的概念，同时不具备 HDFS，只能测试 MapReduce程序。 全分布式集群，就是对每一个必须的节点都采用一个独立的主机，拥有独立的 IP 地址，真正意义的分布式集群概念，完全达到 Hadoop 的实际应用要求。 伪分布式集群，我们学习最常用的一个环境搭建。就是采用一个主机，但是配置多个拥有独立的虚拟节点，满足 Hadoop 分布式的逻辑概念。具备 Hadoop 的所有功能，在单机上模拟一个分布式的环境。这也是我们在学习中建议采用的方式，因为笔者使用的是 Windows7 系统，故而需要借助虚拟机。 搭建 Hadoop 环境准备 安装虚拟机 VMWare、 Linux 操作系统。 配置主机名和 I 静态 IP 地址、免密码登录设置。 约定安装目录： /liwei/hadoop。 配置好 JDK 的环境变量、准备好 Hadoop 安装包。 了解 Hadoop 的目录结构以及对应的作用。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>分布式文件系统</tag>
        <tag>分布式并行计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop之wordcount实例-MapReduce程序]]></title>
    <url>%2FHadoop%E4%B9%8Bwordcount%E5%AE%9E%E4%BE%8B-MapReduce%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[实验目的&emsp;&emsp;利用搭建好的大数据平台 Hadoop，对 HDFS 中的文本文件进行处理，采用 Hadoop Steaming 方式，使用 Python 语言实现英文单词的统计功能，并输出单词统计结果。 实验内容&emsp;&emsp;将附件”COPYING_LGPL.txt”上传 Hadoop 集群的 HDFS 中，采用 Hadoop Steaming方式，使用 Python语言实现字词统计功能，输出字词统计结果，即实现文本单词的词频统计功能。要求将实验原理，过程，代码分析，结果分析记录在实验报告中。 实验步骤 实验原理：&emsp;&emsp;简述 MapReduce 的 Data Flow 如下图所示，原始数据经过 mapper 处理，再进行 partition 和 sort，到达 reducer，输出最后结果。Hadoop 的MapReduce处理框架，一般的编程模型如下图所示， 将一个业务拆分为 Mapper 和 Reducer 两个阶段。使用 Python 语言背后的“技巧”是我们将使用 Hadoop Streaming API 来帮助我们通过 STDIN（标准输入）和 STDOUT（标准输出）在 Map 和 Reduce 代码之间传递数据。我们将简单地使用 Python 的 sys.stdin 来读取输入数据并将我们自己的输出打印到 sys.stdout。这就是我们需要做的全部，因为 Hadoop Streaming 会帮助我们处理其他所有事情！&emsp;&emsp;使用 Python 来调用 Hadoop Streaming API，其基本流程如下图。用 Python 写MapReduce 还需要了解 HadoopStreaming ，在 Apache 的 Hadoop 官网可以查看HadoopStreaming 的运行机制，简单来说就是 HadoopStreaming 是可运行特殊脚本的MapReduce 作业的工具 ，使用格式如下： 12345hadoop jar \/home/hadoop/app/hadoop-2.7.7/share/hadoop/tools/lib/hadoopstreaming-2.7.7.jar\-files /home/hadoop/mapper.py -mapper /home/hadoop/mapper.py \-files /home/hadoop/reducer.py -reducer /home/hadoop/reducer.py \-input /wordcount/COPYING_LGPL.txt -output /wordcount/output 实验过程&emsp;&emsp;将本地物理机的测试文本文件 COPYING_LGPL.txt 上传到虚拟主机 Master 上，在从 Master 上传到 Hadoop 集群的 HDFS 文件系统上/wordcount/COPYING_LGPL.txt。&emsp;&emsp;使用 Python 编写 MapReduce 程序，分别根据实现原理编写 Mapper 程序和Reducer 程序，使用 Vim 编写 Mapper 和 Reducer 脚本，并使两个脚本具有可执行权限，及使用命令： chmod +x mapper.py reducer.py。&emsp;&emsp;使用 HadoopStreaming 命令来运行自己编写的程序，其命令如下： 12345678hadoop jar \/home/hadoop/app/hadoop-2.7.7/share/hadoop/tools/lib/hadoopstreaming-2.7.7.jar \-files /usr/bin/mapper.py \-files /usr/bin/reducer.py \-mapper "python /usr/bin/mapper.py" \-reducer "python /usr/bin/reducer.py" \-input /wordcount/input/COPYING_LGPL.txt \-output /wordcount/output &emsp;&emsp;可以编写一个 shell 脚本命令，来运行 HadoopStreaming 命令，这样在 shell 脚本中首先使用删掉输出目录文件的命令（hdfs dfs -rm -r -f /wordcount/output），防止多次测试出错， 同时每次测试只需要运行 shell 脚本即可，这样在做实验的时候更加方便操作，而不用每次都敲命令。 对HadoopStreaming 命令进行解释： 12345678hadoop jar #指调用hadoop jar包的命令/home/hadoop/app/hadoop-2.7.7/share/hadoop/tools/lib/hadoopstreaming-2.7.7.jar #调用HadoopStreaming 命令的jar包-files /usr/bin/mapper.py #提交的作业的路径-files /usr/bin/reducer.py #提交的作业的路径-mapper "python /usr/bin/mapper.py" #mapper程序的解释器python以及程序路径-reducer "python /usr/bin/reducer.py" #reducer程序的解释器python以及程序路径-input /wordcount/input/COPYING_LGPL.txt #HDFS上的输入文件的路径-output /wordcount/output #HDFS上的输出文件的路径 &emsp;&emsp;HadoopStreaming API 的调用接口说明： 调用 python 中的标准输入流 sys.stdin ，MAP 具体过程是， HadoopStream 每次从 input 文件读取一行数据，然后传到 sys.stdin中，运行 payhon 的 map 函数脚本，然后用 print 输出回 HadoopStreeam。 REDUCE 过程一样。所以 M 和 R 函数的输入格式为 for line in sys.stdin:line=line.strip。Mapper 过程如下： 第一步，在每个节点上运行我们编写的 map 程序 ，即就是 调用标准输入流 ， 读取文本内容，对文本内容分词，形成一个列表，读取列表中每一个元素的值 ， Map 函数输出， key 为 word，下一步将进行 shuffle 过程，将按照key 排序，输出，这两步为 map 阶段工作为，在本地节点进行，第二步， hadoop 框架，把我们运行的结果，进入 shuffle 过程，每个节点对 key 单独进行排序，然后输出。Reducer 过程：第一步， merge 过程，把所有节点汇总到一个节点，合并并且按照 key排序。第二步，运行 reducer 函数。 Python源代码&emsp;&emsp;分析 WordCount 程序实例的实现原理步骤，具体 Python 代码如下源代码所示，前面是简要原理的实现，后面是使用 Python 的迭代器和生成器升级 mapper 程序和 reducer 程序。MapReduce 的 WordCount 简要原理 Python 实现源代码如下Mapper阶段 12345678910111213141516171819#!/usr/bin/env python# filename:mapper.py# date:2019-06-18import sys# input comes from STDIN (standard input)for line in sys.stdin: # remove leading and trailing whitespace line = line.strip() # split the line into words words = line.split() # increase counters for word in words: # write the results to STDOUT (standard output); # what we output here will be the input for the # Reduce step, i.e. the input for reducer.py # tab-delimited; the trivial word count is 1 print '%s\t%s' % (word, 1) Reducer阶段 1234567891011121314151617181920212223242526272829303132333435363738394041#!/usr/bin/env python# filename:reducer.py# date:2019-06-18from operator import itemgetterimport syscurrent_word = Nonecurrent_count = 0word = None# input comes from STDINfor line in sys.stdin: # remove leading and trailing whitespace line = line.strip() # parse the input we got from mapper.py word, count = line.split('\t', 1) # convert count (currently a string) to int try: count = int(count) except ValueError: # count was not a number, so silently # ignore/discard this line continue # this IF-switch only works because Hadoop sorts map output # by key (here: word) before it is passed to the reducer if current_word == word: current_count += count else: if current_word: # write result to STDOUT print '%s\t%s' % (current_word, current_count) current_count = count current_word = word# do not forget to output the last word if needed!if current_word == word: print '%s\t%s' % (current_word, current_count) MapReduce 的 WordCount 简要原理 Python 的迭代器与生成器实现源代码如下：Mapper阶段 1234567891011121314151617181920212223242526#!/usr/bin/env python# filename:mapper.py# date:2019-06-18# detail:A more advanced Mapper, using Python iterators and generators.import sysdef read_input(file): for line in file: # split the line into words yield line.split()def main(separator='\t'): # input comes from STDIN (standard input) data = read_input(sys.stdin) for words in data: # write the results to STDOUT (standard output); # what we output here will be the input for the # Reduce step, i.e. the input for reducer.py # # tab-delimited; the trivial word count is 1 for word in words: print '%s%s%d' % (word, separator, 1)if __name__ == "__main__": main() Reducer阶段 123456789101112131415161718192021222324252627282930#!/usr/bin/env python# filename:reducer.py# date:2019-06-18# detail:A more advanced Reducer, using Python iterators and generators.from itertools import groupbyfrom operator import itemgetterimport sysdef read_mapper_output(file, separator='\t'): for line in file: yield line.rstrip().split(separator, 1)def main(separator='\t'): # input comes from STDIN (standard input) data = read_mapper_output(sys.stdin, separator=separator) # groupby groups multiple word-count pairs by word, # and creates an iterator that returns consecutive keys and their group: # current_word - string containing a word (the key) # group - iterator yielding all ["&amp;lt;current_word&amp;gt;", "&amp;lt;count&amp;gt;"] items for current_word, group in groupby(data, itemgetter(0)): try: total_count = sum(int(count) for current_word, count in group) print "%s%s%d" % (current_word, separator, total_count) except ValueError: # count was not a number, so silently discard this item passif __name__ == "__main__": main()]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>wordcount</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop分布式环境搭建]]></title>
    <url>%2FHadoop%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[实验目的： 熟悉 Linux 操作系统的安装以及常用的基本命令 掌握如何设置静态 IP 地址，掌握如何修改主机域名 掌握如何配置 Java 环境变量，掌握 Java 基本命令 了解为何需要配置 SSH 免密码登录，掌握如何配置 SSH 免密码登录 熟练掌握在 Linux 环境下如何构建分布模式下的 Hadoop 集群 实验内容 安装和配置 CentOS7 的 Liunx 发行版 安装和配置 CentOS7 的网络以及 IP、主机名 启动和配置 SSH 免密码登录，配置 Java 环境 安装和配置 Hadoop 分布式集群环境 实验步骤&emsp;&emsp;分析部署 Hadoop 分布式集群需要三台主机，分别作为三个数据节点DataNode 和三个管理节点NodeManager，再将其中每一个主机分别作为 NameNode、 ResourceManager、SecondaryNameNode。 Master Slave1 slave2 HDFS NameNode DataNode SecondaryNameNode HDFS DataNode DataNode YARN NodeManager ResouceManager NodeManager YARN NodeManager &emsp;&emsp;安装好 VMWare12 虚拟机，新建一个基于 CentOS 发行版的 Liunx 虚拟机，然后安装 CentOS7 操作系统，然后通过克隆方式，克隆出另外两台 CentOS7 主机，以root 权限配置好必要信息后，采用远程登录工具 SecureCRT 登录服务器进行操作。&emsp;&emsp;将建好的虚拟机重新命名为 Hadoop-Node0（主机名为 master）、 HadoopNode1（主机名为 slave1）、 Hadoop-Node2（主机名为 slave2），三台主机都配置一个用户名都为 hadoop，登录密码都为 hadoop，同时使用 root 权限进行对三台主机进行开机关闭防火墙，因为 Hadoop 中的进程端口多而且集群采用内网部署，可以不需要防火墙。&emsp;&emsp;网络配置，使用 VMnet8 这张虚拟网卡，查看网段 IP，网关 IP 地址为192.168.92.1，子网掩码 IP 地址为 255.255.255.0，则在该网段内为三台主机进行配置静态 IP 和重新生成 MAC 地址，使三台主机处于同一网段下， 使用同一时区的时间进行三台主机的时间同步（也可以采用 ntp 的方式进行集群时间同步）， 对应关系如下： Hadoop-Node0——192.168.92.2——master Hadoop-Node1——192.168.92.3——slave1 Hadoop-Node2——192.168.92.4——slave2 &emsp;&emsp;配置好每一台主机的主机名以及静态 IP 地址，同时进行主机名与 IP 地址的映射，在每一台主机的 hosts 文件中都需要配置相同的主机名与 IP 映射，这样才是使用主机名时才能识别主机对应的 IP，文件内容如下： 192.168.92.2 master 192.168.92.3 slave1 192.168.92.4 slave2 &emsp;&emsp;配置 SSH 免密登陆，首先在每台服务器生成密钥对后，即每台机器上都执行 ssh-keygen –t rsa 需要输入密码的地方直接按 Enter 回车键，这样就采用 RSA加密算法生成了公钥秘钥在当前隐藏文件夹.ssh 下。然后在每台服务器上执行 sshcopy-id命令，将公钥复制到其它两台服务器上即可，该命令可以自动将公钥添加到名为 authorized_keys的文件中，在每台服务器都执行完以上步骤后就可以实现多台服务器相互无密码登陆了。 ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@master ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@slave1 ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@lave2 &emsp;&emsp;集群部署，首先要分配好集群的角色，比如 HDFS 的角色有 NN、 DN、SecondaryNN， YARN 的角色有 RM、 NM，分配好在集群中的主机位置。Master Slave1 Slave2三个主机上应该都有着我们提前分配好的角色。分布式文件系统HDFS：NameNode、DataNode、SecondaryNameNode；YARN：NodeManager、ResouceManager。&emsp;&emsp;配置文件，在/home/Hadoop 下创建一个 app/目录，将 hadoop-2.7.7 解压到 app/目录下，将 jdk1-8 也解压到 app/目录下，配置好 JAVA_HOME 和 HADOOP_HOME环境变量，添加到当前用户的 bash_profile。 按照提前准备好的配置文档进行对Hadoop 进行文件的配置，在/home/hadoop/app/hadoop-2.7.7/etc/hadoop/对各个配置文件进行修改配置。&emsp;&emsp;对 HDFS 进行格式化，在 master 主机上进行格式化即可。使用 hdfs namenode-format 命令进行格式化，等待格式化的结果，会显示成功的格式化目录在配置好的/home/hadoop/app/hadoop-2.7.7/temp 目录下。 &emsp;&emsp;启动 Hadoop 的组件的进程（在 master 主机上启动 HDFS、 在 slave1 主机上启 动 YARN ） ， 采 用 jps 命 令 查 看 进 程 和 通 过 浏 览 器 查 看 。 &emsp;&emsp;上传文件到 HDFS 后，通过浏览器查看目录文件以及分块情况，同时也可以在 Liunx 端查看到情况。&emsp;&emsp;运行 wordcount 程序，体验并检验 Hadoop 集群。&emsp;&emsp;关闭 Hadoop 集群，现在 master 主机上关闭 HDFS，在 slave1 主机上关闭YARN，则 slave2 主机上的 Hadoop 集群的进程就自动被关闭了。 实验代码或分析重启网卡 1systemctl restart network 查看防火墙状态 1systemctl status firewalld 设置开机不启动防火墙 1systemctl disable firewalld 配置免密登录先在每一台主机上执行生成 RSA 算法的秘钥 1ssh-keygen –t rsa 从 master 以 hadoop 用户登录 master、 slave1、 slave2 免密在 master 主机上进行一下操作命令即可 123ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@masterssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@slave1ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@slave2 从 slave1 以 hadoop 用户登录 master、 slave1、 slave2 免密在 slave1 主机上进行一下操作命令即可 123ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@masterssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@slave1ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@slave2 从 slave2 以 hadoop 用户登录 master、 slave1、 slave2 免密在 slave2 主机上进行一下操作命令即可 123ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@masterssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@slave1ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@slave226. 配置好 master 主机上的 Hadoop 后进行集群分发到 slave1 和 slave2 12scp –r /home/hadoop/app hadoop@slaver1:/home/hadoopscp –r /home/hadoop/app hadoop@slaver2:/home/hadoop 分发 master 主机上的环境变量配置文件 12scp –r ~/.bash_profile hadoop@slaver1:~/scp –r ~/.bash_profile hadoop@slaver2:~/ Hadoop配置文件 核心配置文件 /home/hadoop/app/hadoop-2.7.7/etc/hadoop/slaves 1234&lt;!--配置 Hadoop 集群主机--&gt;masterslave1slave2 /home/hadoop/app/hadoop-2.7.7/etc/hadoop/core-site.xml 123456789101112&lt;configuration&gt; &lt;!--配置 HDFS 的 NameNode--&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;!--配置 DataNode 保存数据的位置--&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.7.7/temp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; HDFS 配置文件 /home/hadoop/app/hadoop-2.7.7/etc/hadoop/hadoop-env.sh 12&lt;!--配置 HDFS 的 Java 环境--&gt;export JAVA_HOME=/home/hadoop/app/jdk1.8.0_191 /home/hadoop/app/hadoop-2.7.7/etc/hadoop/hdfs-site.xml 1234567891011121314151617&lt;configuration&gt; &lt;!--配置 HDFS 的副本数--&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!--配置是否检查权限--&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!--配置 Hadoop 辅助名称节点主机配置 SecondaryNameNode--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;slave2:50090&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; YARN 配置文件 /home/hadoop/app/hadoop-2.7.7/etc/hadoop/yarn-env.sh 12&lt;!--配置 YARN 的 Java 环境--&gt;JAVA_HOME=/home/hadoop/app/jdk1.8.0_191 /home/hadoop/app/hadoop-2.7.7/etc/hadoop/yarn-site.xml 123456789101112&lt;configuration&gt; &lt;!--配置 ResourceManager 的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;slave1&lt;/value&gt; &lt;/property&gt; &lt;!--配置 NodeManager 执行任务的方式--&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; MapReduce 配置文件 /home/hadoop/app/hadoop-2.7.7/etc/hadoop/mapred-env.sh 12&lt;!--配置 MapReduce 的 Java 环境--&gt;export JAVA_HOME=/home/hadoop/app/jdk1.8.0_191 /home/hadoop/app/hadoop-2.7.7/etc/hadoop/mapred-site.xml.template先修改文件 MapReduce 1cp mapred-site.xml.template mapred-site.xml /home/hadoop/app/hadoop-2.7.7/etc/hadoop/mapred-site.xml &lt;configuration&gt; &lt;!--配置 MapReduce 运行在 YARN 上--&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 实验检测上传/home/hadoop/hadoop-2.7.7.tar.gz 和jdk-8u191-linux-x64.tar.gz 文件到 HDFS 根目录 hdfs dfs -put /home/hadoop/hadoop-2.7.7.tar.gz / hdfs dfs -put /home/hadoop/jdk-8u191-linux-x64.tar.gz / 以递归方式查看 HDFS 的根目录结构 hdfs dfs -ls -R / 运行 Hadoop 自带的 wordcount 程序，进行词频统计 hadoop jar hadoop-mapreduce-examples-2.7.7.jar wordcount /README.txt /output hadoop jar hadoop-mapreduce-examples-2.7.7.jar wordcount /LICENSE.txt /output 注意说明&emsp;&emsp;通过本次部署大数据平台 Hadoop 的分布式环境，比之前部署伪分布式环境更加熟悉整个过程的搭建，进一步的深刻理解 Liunx 的操作基础以及 Hadoop 的分布式环境搭建，对Hadoop的认识更加的深刻理解。&emsp;&emsp;对 Liunx 的网络配置以及整个集群的静态 IP 设置和网关设置、防火墙的状态查看以及关闭，以及主机名设置和主机名与 IP 进行映射，通过配置 SSH 的非对称加密，通过公钥和私钥实现三台主机之间的相互之间免密登陆。&emsp;&emsp;配置 Hadoop 的核心组件，核心配置文件 core-site.xml，配置 HDFS 的NameNode 地址以及运行时储存目录； HDFS 配置文件 hadoop-env.sh 用于配置 HDFS的 Java 环境， hdfs-site.xml 指定副本数以及辅助名称节点的主机配置； YARN 配置文件 yarn-env.sh 用于配置 YARN 的 Java 环境， yarn-site.xml 配置 YARN 的NodeManger 和 ResourceManger； MapReduce 配置文件 mapred-env.sh 用于配置MapReduce 的 Java 环境， mapred-site.xml 配置 MapReduce 运行在 YARN 上。&emsp;&emsp;采用分发 scp 命令或者采用脚本进行分发集群搭建，以及了解使用 rsync对集群中存在差异的配置文件进行同步更新，以及在集群中常用的时间同步方法以及了解采用部署 ntp 集群实现时间同步。&emsp;&emsp;进一步理解 Hadoop 的各个组件原理，特别是 HDFS 的储存原理，分块储存以及副本机制等等。使用提供的 jar 包，运行 wordcount 程序 jar 包，体验大数据Hadoop 分布式平台以及 MapReduce 数据处理框架。&emsp;&emsp;对于每一个操作步骤不是很清楚的，可以访问本人的GitHub，在学习资料里面有着详细的学习以及安装的过程，是PDF格式的文档说明。 Hadoop分布式环境搭建详细文档]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>分布式环境</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo搭建博客]]></title>
    <url>%2FHexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[Hexo简介&emsp;&emsp;Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用Markdown（或其他渲染引擎）解析文章，在极短的几秒钟内，就可利用靓丽的主题生成静态网页，特别适合搭建个人博客。Hexo以其简单、高效而且主题丰富多彩而著名，迅速地占据了一部分市场，值得尝试。&emsp;&emsp;本博客就是采用Hexo+GitHubPages+NexT主题进行搭建的！ Hexo官网 GitHub官网 NexT官网 安装环境&emsp;&emsp;Hexo在搭建时需要Node.js的环境支持，同时需要命令终端的支持。针对不同的操作系统Windows、Linux或者Mac，在配置搭建环境时要下载安装对应的版本，在Windows系统下，建议使用GitBash。Mac和linux都是自带的有BashShell终端。&emsp;&emsp;由于Node.js是国外的网站，有时候由于网络的一些因素，容易影响我们采用npm对资源的访问速度，故此可以采用国内的阿里巴巴的镜像，可以加快访问的速度以及效率。首先在终端采用npm命令安装镜像，成功后就可以采用cnpm命令代替npm命令了。 npm install -g cnpm –registry=https://registry.npm.taobao.org Node.js Git 安装Hexo&emsp;&emsp;在下载、安装并配置好环境后，就可以进行安装Hexo。首先打开BashShell终端，创建一个空的目录来作为整个博客项目的工作空间，然后再使用命令进行安装hexo，安装成功后就对Hexo进行初始化，初始化成功后可以查看整个工作空间的目录结构和文件树。 创建一个空文件夹blog：$ mkdir blog 在终端使用命令安装Hexo：$ npm install -g hexo-cli 初始化Hexo：$ hexo init 查看hexo生成的目录:$ cd blog$ ls 配置NexT主题&emsp;&emsp;在Hexo的官网可以找到许许多多的主题，寻找适合自己喜欢的主题，可以通过BashShell终端进行下载，也可以下载后解压到站点的主题文件夹下即可，然后打开站点的配置文件，搜索到theme将其值修改为自己下载的主题名即可。 在终端使用命令下载主题：$ git clone https://github.com/theme-next/hexo-theme-next themes/next ./blog/themes/ 修改配置文件_config.yml:$ vim _config.yml将theme的值由landscape修改为hexo-theme-next即可 配置GitHub的Page：&emsp;&emsp;首先要有一个GitHub账号，其次创建一个规定的GitHubPages主页仓库。然后就可以对站点的配置文件进行修改了，添加如下内容即可。而且不仅可以使用GitHub的Pages进行托管，也可以使用国内的Coding的Pages进行托管，同时也可以使用两者进行负载均衡，进行国内外的分流托管。 修改配置文件_config：在文件末尾的deploy添加内容：deploy:type: gitrepo: git@github.com:2694048168/2694048168.github.io.gitbranch: master 写博客文章&emsp;&emsp;编写自己的博客文章(采用markdown格式)，可以在终端采用Vim进行编写，也可以用Notepad++类似的编辑器编写，只要博客文章所在的路径是正确的，就可以被Hexo识别并读取到即可。 在终端命令创建文章：$ hexo new file_name 使用Vim等编辑器编辑文章 更新Hexo并上传&emsp;&emsp;在更新上传之前可以在本地进行预览一下，即就是先清除clean、生成generate、然后start启动本地，在本地的4000端口查看即可。没有问题后，在进行上传deploy。当然这些常用的命令都是可以编写一个shell脚本进行的，因为每次都需要的，强烈建议写成一个shell脚本。 在终端依次使用命令：$ hexo clean$ hexo g$ hexo d 终端访问&emsp;&emsp;通过PC端浏览器或者智能终端浏览器访问即可。 本地访问：http://localhost:4000 访问地址：https://2694048168.github.io 注意说明 在Windows系统下，终端采用GitBash即可 在Linux系统下，终端采用自带的Bash即可 Linux系统用户需要注意命令的权限问题 在Mac系统下，终端采用自带的Bash即可 整个操作过程全部都在blog目录下，注意操作命令的路径问题 博客文章格式采用Markdown hexo s 命令是启动本地hexo，访问通过http://localhost:4000 关于github的page详情查看：github.pages]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub</tag>
        <tag>NexT</tag>
      </tags>
  </entry>
</search>
