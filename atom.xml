<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>云主宰苍穹</title>
  
  <subtitle>Stay Hungry, Stay Foolish.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="/"/>
  <updated>2019-08-12T05:32:32.231Z</updated>
  <id>/</id>
  
  <author>
    <name>云主宰苍穹</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hadoop之wordcount实例-MapReduce程序</title>
    <link href="/Hadoop%E4%B9%8Bwordcount%E5%AE%9E%E4%BE%8B-MapReduce%E7%A8%8B%E5%BA%8F/2019/08/11/"/>
    <id>/Hadoop之wordcount实例-MapReduce程序/2019/08/11/</id>
    <published>2019-08-11T15:24:08.000Z</published>
    <updated>2019-08-12T05:32:32.231Z</updated>
    
    <content type="html"><![CDATA[<h3 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h3><p>&emsp;&emsp;利用搭建好的大数据平台 Hadoop，对 HDFS 中的文本文件进行处理，采用 Hadoop Steaming 方式，使用 Python 语言实现英文单词的统计功能，并输出单词统计结果。</p><!--图片链接--><p><img src="/images/MapReduce-wordcount.png" alt="self"></p><h3 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h3><p>&emsp;&emsp;将附件”COPYING_LGPL.txt”上传 Hadoop 集群的 HDFS 中，采用 Hadoop Steaming方式，使用 Python语言实现字词统计功能，输出字词统计结果，即实现文本单词的词频统计功能。要求将实验原理，过程，代码分析，结果分析记录在实验报告中。</p><h3 id="实验步骤"><a href="#实验步骤" class="headerlink" title="实验步骤"></a>实验步骤</h3><ul><li>实验原理：<br>&emsp;&emsp;简述 MapReduce 的 Data Flow 如下图所示，原始数据经过 mapper 处理，再进行 partition 和 sort，到达 reducer，输出最后结果。Hadoop 的MapReduce处理框架，一般的编程模型如下图所示， 将一个业务拆分为 Mapper 和 Reducer 两个阶段。使用 Python 语言背后的“技巧”是我们将使用 Hadoop Streaming API 来帮助我们通过 STDIN（标准输入）和 STDOUT（标准输出）在 Map 和 Reduce 代码之间传递数据。我们将简单地使用 Python 的 sys.stdin 来读取输入数据并将我们自己的输出打印到 sys.stdout。这就是我们需要做的全部，因为 Hadoop Streaming 会帮助我们处理其他所有事情！<br>&emsp;&emsp;使用 Python 来调用 Hadoop Streaming API，其基本流程如下图。用 Python 写MapReduce 还需要了解 HadoopStreaming ，在 Apache 的 Hadoop 官网可以查看HadoopStreaming 的运行机制，简单来说就是 HadoopStreaming 是可运行特殊脚本的MapReduce 作业的工具 ，使用格式如下：<!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar \</span><br><span class="line">/home/hadoop/app/hadoop-2.7.7/share/hadoop/tools/lib/hadoopstreaming-2.7.7.jar\</span><br><span class="line">-files /home/hadoop/mapper.py -mapper /home/hadoop/mapper.py \</span><br><span class="line">-files /home/hadoop/reducer.py -reducer /home/hadoop/reducer.py \</span><br><span class="line">-input /wordcount/COPYING_LGPL.txt -output /wordcount/output</span><br></pre></td></tr></table></figure></li></ul><!--图片链接--><p><img src="/images/MapReduce.png" alt="self"></p><ul><li>实验过程<br>&emsp;&emsp;将本地物理机的测试文本文件 COPYING_LGPL.txt 上传到虚拟主机 Master 上，在从 Master 上传到 Hadoop 集群的 HDFS 文件系统上/wordcount/COPYING_LGPL.txt。<br>&emsp;&emsp;使用 Python 编写 MapReduce 程序，分别根据实现原理编写 Mapper 程序和Reducer 程序，使用 Vim 编写 Mapper 和 Reducer 脚本，并使两个脚本具有可执行权限，及使用命令： chmod +x mapper.py reducer.py。<br>&emsp;&emsp;使用 HadoopStreaming 命令来运行自己编写的程序，其命令如下：<!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar \</span><br><span class="line">/home/hadoop/app/hadoop-2.7.7/share/hadoop/tools/lib/hadoopstreaming-2.7.7.jar \</span><br><span class="line">-files /usr/bin/mapper.py \</span><br><span class="line">-files /usr/bin/reducer.py \</span><br><span class="line">-mapper "python /usr/bin/mapper.py" \</span><br><span class="line">-reducer "python /usr/bin/reducer.py" \</span><br><span class="line">-input /wordcount/input/COPYING_LGPL.txt \</span><br><span class="line">-output /wordcount/output</span><br></pre></td></tr></table></figure></li></ul><p>&emsp;&emsp;可以编写一个 shell 脚本命令，来运行 HadoopStreaming 命令，这样在 shell 脚本<br>中首先使用删掉输出目录文件的命令（hdfs dfs -rm -r -f /wordcount/output），防止多次测试出错， 同时每次测试只需要运行 shell 脚本即可，这样在做实验的时候更加方便操作，而不用每次都敲命令。</p><h3 id="对HadoopStreaming-命令进行解释："><a href="#对HadoopStreaming-命令进行解释：" class="headerlink" title="对HadoopStreaming 命令进行解释："></a>对HadoopStreaming 命令进行解释：</h3><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar    #指调用hadoop jar包的命令</span><br><span class="line">/home/hadoop/app/hadoop-2.7.7/share/hadoop/tools/lib/hadoopstreaming-2.7.7.jar              #调用HadoopStreaming 命令的jar包</span><br><span class="line">-files /usr/bin/mapper.py     #提交的作业的路径</span><br><span class="line">-files /usr/bin/reducer.py    #提交的作业的路径</span><br><span class="line">-mapper "python /usr/bin/mapper.py"     #mapper程序的解释器python以及程序路径</span><br><span class="line">-reducer "python /usr/bin/reducer.py"    #reducer程序的解释器python以及程序路径</span><br><span class="line">-input /wordcount/input/COPYING_LGPL.txt     #HDFS上的输入文件的路径</span><br><span class="line">-output /wordcount/output     #HDFS上的输出文件的路径</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;HadoopStreaming API 的调用接口说明： 调用 python 中的标准输入流 sys.stdin ，MAP 具体过程是， HadoopStream 每次从 input 文件读取一行数据，然后传到 sys.stdin中，运行 payhon 的 map 函数脚本，然后用 print 输出回 HadoopStreeam。 REDUCE 过程一样。所以 M 和 R 函数的输入格式为 for line in sys.stdin:line=line.strip。Mapper 过程如下： 第一步，在每个节点上运行我们编写的 map 程序 ，即就是 调用标准输入流 ， 读取文本内容，对文本内容分词，形成一个列表，读取列表中每一个元素的值 ， Map 函数输出， key 为 word，下一步将进行 shuffle 过程，将按照key 排序，输出，这两步为 map 阶段工作为，在本地节点进行，第二步， hadoop 框架，把我们运行的结果，进入 shuffle 过程，每个节点对 key 单独进行排序，然后输出。Reducer 过程：第一步， merge 过程，把所有节点汇总到一个节点，合并并且按照 key排序。第二步，运行 reducer 函数。</p><!--图片链接--><p><img src="/images/WordCount%E7%9A%84%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90.png" alt="self"></p><h3 id="Python源代码"><a href="#Python源代码" class="headerlink" title="Python源代码"></a>Python源代码</h3><p>&emsp;&emsp;分析 WordCount 程序实例的实现原理步骤，具体 Python 代码如下源代码所示，前面是简要原理的实现，后面是使用 Python 的迭代器和生成器升级 mapper 程序和 reducer 程序。<br><strong>MapReduce 的 WordCount 简要原理 Python 实现源代码如下</strong><br><strong>Mapper阶段</strong></p><!--Code--><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># filename:mapper.py</span></span><br><span class="line"><span class="comment"># date:2019-06-18</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># input comes from STDIN (standard input)</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    <span class="comment"># remove leading and trailing whitespace</span></span><br><span class="line">    line = line.strip()</span><br><span class="line">    <span class="comment"># split the line into words</span></span><br><span class="line">    words = line.split()</span><br><span class="line">    <span class="comment"># increase counters</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="comment"># write the results to STDOUT (standard output);</span></span><br><span class="line">        <span class="comment"># what we output here will be the input for the</span></span><br><span class="line">        <span class="comment"># Reduce step, i.e. the input for reducer.py</span></span><br><span class="line">        <span class="comment"># tab-delimited; the trivial word count is 1</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'%s\t%s'</span> % (word, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><strong>Reducer阶段</strong></p><!--Code--><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># filename:reducer.py</span></span><br><span class="line"><span class="comment"># date:2019-06-18</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">current_word = <span class="literal">None</span></span><br><span class="line">current_count = <span class="number">0</span></span><br><span class="line">word = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># input comes from STDIN</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    <span class="comment"># remove leading and trailing whitespace</span></span><br><span class="line">    line = line.strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse the input we got from mapper.py</span></span><br><span class="line">    word, count = line.split(<span class="string">'\t'</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert count (currently a string) to int</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        count = int(count)</span><br><span class="line">    <span class="keyword">except</span> ValueError:</span><br><span class="line">        <span class="comment"># count was not a number, so silently</span></span><br><span class="line">        <span class="comment"># ignore/discard this line</span></span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># this IF-switch only works because Hadoop sorts map output</span></span><br><span class="line">    <span class="comment"># by key (here: word) before it is passed to the reducer</span></span><br><span class="line">    <span class="keyword">if</span> current_word == word:</span><br><span class="line">        current_count += count</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> current_word:</span><br><span class="line">            <span class="comment"># write result to STDOUT</span></span><br><span class="line">            <span class="keyword">print</span> <span class="string">'%s\t%s'</span> % (current_word, current_count)</span><br><span class="line">        current_count = count</span><br><span class="line">        current_word = word</span><br><span class="line"></span><br><span class="line"><span class="comment"># do not forget to output the last word if needed!</span></span><br><span class="line"><span class="keyword">if</span> current_word == word:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'%s\t%s'</span> % (current_word, current_count)</span><br></pre></td></tr></table></figure><p><strong>MapReduce 的 WordCount 简要原理 Python 的迭代器与生成器实现源代码如下：</strong><br><strong>Mapper阶段</strong></p><!--Code--><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># filename:mapper.py</span></span><br><span class="line"><span class="comment"># date:2019-06-18</span></span><br><span class="line"><span class="comment"># detail:A more advanced Mapper, using Python iterators and generators.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_input</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file:</span><br><span class="line">        <span class="comment"># split the line into words</span></span><br><span class="line">        <span class="keyword">yield</span> line.split()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(separator=<span class="string">'\t'</span>)</span>:</span></span><br><span class="line">    <span class="comment"># input comes from STDIN (standard input)</span></span><br><span class="line">    data = read_input(sys.stdin)</span><br><span class="line">    <span class="keyword">for</span> words <span class="keyword">in</span> data:</span><br><span class="line">        <span class="comment"># write the results to STDOUT (standard output);</span></span><br><span class="line">        <span class="comment"># what we output here will be the input for the</span></span><br><span class="line">        <span class="comment"># Reduce step, i.e. the input for reducer.py</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># tab-delimited; the trivial word count is 1</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'%s%s%d'</span> % (word, separator, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p><strong>Reducer阶段</strong></p><!--Code--><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># filename:reducer.py</span></span><br><span class="line"><span class="comment"># date:2019-06-18</span></span><br><span class="line"><span class="comment"># detail:A more advanced Reducer, using Python iterators and generators.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> groupby</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_mapper_output</span><span class="params">(file, separator=<span class="string">'\t'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file:</span><br><span class="line">        <span class="keyword">yield</span> line.rstrip().split(separator, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(separator=<span class="string">'\t'</span>)</span>:</span></span><br><span class="line">    <span class="comment"># input comes from STDIN (standard input)</span></span><br><span class="line">    data = read_mapper_output(sys.stdin, separator=separator)</span><br><span class="line">    <span class="comment"># groupby groups multiple word-count pairs by word,</span></span><br><span class="line">    <span class="comment"># and creates an iterator that returns consecutive keys and their group:</span></span><br><span class="line">    <span class="comment">#   current_word - string containing a word (the key)</span></span><br><span class="line">    <span class="comment">#   group - iterator yielding all ["&amp;lt;current_word&amp;gt;", "&amp;lt;count&amp;gt;"] items</span></span><br><span class="line">    <span class="keyword">for</span> current_word, group <span class="keyword">in</span> groupby(data, itemgetter(<span class="number">0</span>)):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            total_count = sum(int(count) <span class="keyword">for</span> current_word, count <span class="keyword">in</span> group)</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"%s%s%d"</span> % (current_word, separator, total_count)</span><br><span class="line">        <span class="keyword">except</span> ValueError:</span><br><span class="line">            <span class="comment"># count was not a number, so silently discard this item</span></span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;实验目的&quot;&gt;&lt;a href=&quot;#实验目的&quot; class=&quot;headerlink&quot; title=&quot;实验目的&quot;&gt;&lt;/a&gt;实验目的&lt;/h3&gt;&lt;p&gt;&amp;emsp;&amp;emsp;利用搭建好的大数据平台 Hadoop，对 HDFS 中的文本文件进行处理，采用 Hadoop Ste
      
    
    </summary>
    
      <category term="Hadoop" scheme="/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="/tags/Hadoop/"/>
    
      <category term="wordcount" scheme="/tags/wordcount/"/>
    
      <category term="MapReduce" scheme="/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop分布式环境搭建</title>
    <link href="/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/2019/08/11/"/>
    <id>/Hadoop分布式环境搭建/2019/08/11/</id>
    <published>2019-08-11T14:53:26.000Z</published>
    <updated>2019-08-12T05:58:46.592Z</updated>
    
    <content type="html"><![CDATA[<h3 id="实验目的："><a href="#实验目的：" class="headerlink" title="实验目的："></a>实验目的：</h3><ul><li>熟悉 Linux 操作系统的安装以及常用的基本命令</li><li>掌握如何设置静态 IP 地址，掌握如何修改主机域名</li><li>掌握如何配置 Java 环境变量，掌握 Java 基本命令</li><li>了解为何需要配置 SSH 免密码登录，掌握如何配置 SSH 免密码登录</li><li>熟练掌握在 Linux 环境下如何构建分布模式下的 Hadoop 集群</li></ul><h3 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h3><ul><li>安装和配置 CentOS7 的 Liunx 发行版</li><li>安装和配置 CentOS7 的网络以及 IP、主机名</li><li>启动和配置 SSH 免密码登录，配置 Java 环境</li><li>安装和配置 Hadoop 分布式集群环境</li></ul><h3 id="实验步骤"><a href="#实验步骤" class="headerlink" title="实验步骤"></a>实验步骤</h3><p>&emsp;&emsp;分析部署 Hadoop 分布式集群需要三台主机，分别作为三个数据节点DataNode 和三个管理节点NodeManager，再将其中每一个主机分别作为 NameNode、 ResourceManager、SecondaryNameNode。</p><!--Table--><table><thead><tr><th align="center"></th><th align="center">Master</th><th align="center">Slave1</th><th align="center">slave2</th></tr></thead><tbody><tr><td align="center">HDFS</td><td align="center">NameNode</td><td align="center">DataNode</td><td align="center">SecondaryNameNode</td></tr><tr><td align="center">HDFS</td><td align="center">DataNode</td><td align="center"></td><td align="center">DataNode</td></tr><tr><td align="center">YARN</td><td align="center">NodeManager</td><td align="center">ResouceManager</td><td align="center">NodeManager</td></tr><tr><td align="center">YARN</td><td align="center"></td><td align="center">NodeManager</td><td align="center"></td></tr></tbody></table><p>&emsp;&emsp;安装好 VMWare12 虚拟机，新建一个基于 CentOS 发行版的 Liunx 虚拟机，然后安装 CentOS7 操作系统，然后通过克隆方式，克隆出另外两台 CentOS7 主机，以root 权限配置好必要信息后，采用远程登录工具 SecureCRT 登录服务器进行操作。<br>&emsp;&emsp;将建好的虚拟机重新命名为 Hadoop-Node0（主机名为 master）、 HadoopNode1（主机名为 slave1）、 Hadoop-Node2（主机名为 slave2），三台主机都配置一个用户名都为 hadoop，登录密码都为 hadoop，同时使用 root 权限进行对三台主机进行开机关闭防火墙，因为 Hadoop 中的进程端口多而且集群采用内网部署，可以不需要防火墙。<br>&emsp;&emsp;网络配置，使用 VMnet8 这张虚拟网卡，查看网段 IP，网关 IP 地址为192.168.92.1，子网掩码 IP 地址为 255.255.255.0，则在该网段内为三台主机进行配置静态 IP 和重新生成 MAC 地址，使三台主机处于同一网段下， 使用同一时区的时间进行三台主机的时间同步（也可以采用 ntp 的方式进行集群时间同步）， 对应关系如下：</p><ul><li>Hadoop-Node0——192.168.92.2——master</li><li>Hadoop-Node1——192.168.92.3——slave1</li><li>Hadoop-Node2——192.168.92.4——slave2</li></ul><!--图片链接--><p><img src="/images/VMWare%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE.png" alt="self"><br>&emsp;&emsp;配置好每一台主机的主机名以及静态 IP 地址，同时进行主机名与 IP 地址的映射，在每一台主机的 hosts 文件中都需要配置相同的主机名与 IP 映射，这样才是使用主机名时才能识别主机对应的 IP，文件内容如下：</p><ul><li>192.168.92.2 master</li><li>192.168.92.3 slave1</li><li>192.168.92.4 slave2</li></ul><p>&emsp;&emsp;配置 SSH 免密登陆，首先在每台服务器生成密钥对后，即每台机器上都执行 ssh-keygen –t rsa 需要输入密码的地方直接按 Enter 回车键，这样就采用 RSA加密算法生成了公钥秘钥在当前隐藏文件夹.ssh 下。然后在每台服务器上执行 sshcopy-id命令，将公钥复制到其它两台服务器上即可，该命令可以自动将公钥添加到名为 authorized_keys的文件中，在每台服务器都执行完以上步骤后就可以实现多台服务器相互无密码登陆了。</p><ul><li>ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@master</li><li>ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@slave1</li><li>ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@lave2</li></ul><!--图片链接--><p><img src="/images/%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95.png" alt="self"><br>&emsp;&emsp;集群部署，首先要分配好集群的角色，比如 HDFS 的角色有 NN、 DN、SecondaryNN， YARN 的角色有 RM、 NM，分配好在集群中的主机位置。Master Slave1 Slave2三个主机上应该都有着我们提前分配好的角色。分布式文件系统HDFS：NameNode、DataNode、SecondaryNameNode；YARN：NodeManager、ResouceManager。<br>&emsp;&emsp;配置文件，在/home/Hadoop 下创建一个 app/目录，将 hadoop-2.7.7 解压到 app/目录下，将 jdk1-8 也解压到 app/目录下，配置好 JAVA_HOME 和 HADOOP_HOME环境变量，添加到当前用户的 bash_profile。 按照提前准备好的配置文档进行对Hadoop 进行文件的配置，在/home/hadoop/app/hadoop-2.7.7/etc/hadoop/对各个配置文件进行修改配置。<br>&emsp;&emsp;对 HDFS 进行格式化，在 master 主机上进行格式化即可。使用 hdfs namenode-format 命令进行格式化，等待格式化的结果，会显示成功的格式化目录在配置好的/home/hadoop/app/hadoop-2.7.7/temp 目录下。</p><!--图片链接--><p><img src="/images/hdfs%E6%A0%BC%E5%BC%8F%E5%8C%96.png" alt="self"><br>&emsp;&emsp;启动 Hadoop 的组件的进程（在 master 主机上启动 HDFS、 在 slave1 主机上启 动 YARN ） ， 采 用 jps 命 令 查 看 进 程 和 通 过 浏 览 器 查 看 。</p><!--图片链接--><p><img src="/images/start-hdfs.png" alt="self"></p><!--图片链接--><p><img src="/images/start-yarn.png" alt="self"><br>&emsp;&emsp;上传文件到 HDFS 后，通过浏览器查看目录文件以及分块情况，同时也可以在 Liunx 端查看到情况。<br>&emsp;&emsp;运行 wordcount 程序，体验并检验 Hadoop 集群。<br>&emsp;&emsp;关闭 Hadoop 集群，现在 master 主机上关闭 HDFS，在 slave1 主机上关闭YARN，则 slave2 主机上的 Hadoop 集群的进程就自动被关闭了。</p><!--图片链接--><p><img src="/images/stopHDFS.png" alt="self"></p><!--图片链接--><p><img src="/images/stopYARN.png" alt="self"></p><h3 id="实验代码或分析"><a href="#实验代码或分析" class="headerlink" title="实验代码或分析"></a>实验代码或分析</h3><p>重启网卡</p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart network</span><br></pre></td></tr></table></figure><p>查看防火墙状态</p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status firewalld</span><br></pre></td></tr></table></figure><p>设置开机不启动防火墙</p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure><p>配置免密登录<br>先在每一台主机上执行生成 RSA 算法的秘钥</p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen –t rsa</span><br></pre></td></tr></table></figure><p>从 master 以 hadoop 用户登录 master、 slave1、 slave2 免密<br>在 master 主机上进行一下操作命令即可</p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@master</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@slave1</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@slave2</span><br></pre></td></tr></table></figure><p>从 slave1 以 hadoop 用户登录 master、 slave1、 slave2 免密<br>在 slave1 主机上进行一下操作命令即可</p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@master</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@slave1</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@slave2</span><br></pre></td></tr></table></figure><p>从 slave2 以 hadoop 用户登录 master、 slave1、 slave2 免密<br>在 slave2 主机上进行一下操作命令即可</p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@master</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@slave1</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@slave226.</span><br></pre></td></tr></table></figure><p>配置好 master 主机上的 Hadoop 后进行集群分发到 slave1 和 slave2</p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp –r /home/hadoop/app hadoop@slaver1:/home/hadoop</span><br><span class="line">scp –r /home/hadoop/app hadoop@slaver2:/home/hadoop</span><br></pre></td></tr></table></figure><p>分发 master 主机上的环境变量配置文件</p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp –r ~/.bash_profile hadoop@slaver1:~/</span><br><span class="line">scp –r ~/.bash_profile hadoop@slaver2:~/</span><br></pre></td></tr></table></figure><h3 id="Hadoop配置文件"><a href="#Hadoop配置文件" class="headerlink" title="Hadoop配置文件"></a>Hadoop配置文件</h3><ul><li>核心配置文件</li></ul><p><strong>/home/hadoop/app/hadoop-2.7.7/etc/hadoop/slaves</strong></p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--配置 Hadoop 集群主机--&gt;</span><br><span class="line">master</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><p><strong>/home/hadoop/app/hadoop-2.7.7/etc/hadoop/core-site.xml</strong></p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!--配置 HDFS 的 NameNode--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!--配置 DataNode 保存数据的位置--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/app/hadoop-2.7.7/temp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><ul><li>HDFS 配置文件</li></ul><p><strong>/home/hadoop/app/hadoop-2.7.7/etc/hadoop/hadoop-env.sh</strong></p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--配置 HDFS 的 Java 环境--&gt;</span><br><span class="line">export JAVA_HOME=/home/hadoop/app/jdk1.8.0_191</span><br></pre></td></tr></table></figure><p><strong>/home/hadoop/app/hadoop-2.7.7/etc/hadoop/hdfs-site.xml</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!--配置 HDFS 的副本数--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!--配置是否检查权限--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.permissions&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!--配置 Hadoop 辅助名称节点主机配置 SecondaryNameNode--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;slave2:50090&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><ul><li>YARN 配置文件</li></ul><p><strong>/home/hadoop/app/hadoop-2.7.7/etc/hadoop/yarn-env.sh</strong></p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--配置 YARN 的 Java 环境--&gt;</span><br><span class="line">JAVA_HOME=/home/hadoop/app/jdk1.8.0_191</span><br></pre></td></tr></table></figure><p> <strong>/home/hadoop/app/hadoop-2.7.7/etc/hadoop/yarn-site.xml</strong></p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!--配置 ResourceManager 的地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;slave1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!--配置 NodeManager 执行任务的方式--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><ul><li>MapReduce 配置文件</li></ul><p><strong>/home/hadoop/app/hadoop-2.7.7/etc/hadoop/mapred-env.sh</strong></p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--配置 MapReduce 的 Java 环境--&gt;</span><br><span class="line">export JAVA_HOME=/home/hadoop/app/jdk1.8.0_191</span><br></pre></td></tr></table></figure><p><strong>/home/hadoop/app/hadoop-2.7.7/etc/hadoop/mapred-site.xml.template</strong><br>先修改文件 MapReduce</p><!--Code--><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure><p><strong>/home/hadoop/app/hadoop-2.7.7/etc/hadoop/mapred-site.xml</strong></p><!--Code--><pre><code class="shell">&lt;configuration&gt;    &lt;!--配置 MapReduce 运行在 YARN 上--&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;        &lt;value&gt;yarn&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h3 id="实验检测"><a href="#实验检测" class="headerlink" title="实验检测"></a>实验检测</h3><p>上传/home/hadoop/hadoop-2.7.7.tar.gz 和jdk-8u191-linux-x64.tar.gz 文件到 HDFS 根目录</p><!--Code--><pre><code class="shell">hdfs dfs -put /home/hadoop/hadoop-2.7.7.tar.gz /hdfs dfs -put /home/hadoop/jdk-8u191-linux-x64.tar.gz /</code></pre><p>以递归方式查看 HDFS 的根目录结构</p><!--Code--><pre><code class="shell">hdfs dfs -ls -R /</code></pre><p>运行 Hadoop 自带的 wordcount 程序，进行词频统计</p><!--Code--><pre><code class="shell">hadoop jar hadoop-mapreduce-examples-2.7.7.jar wordcount /README.txt /outputhadoop jar hadoop-mapreduce-examples-2.7.7.jar wordcount /LICENSE.txt /output</code></pre><h3 id="注意说明"><a href="#注意说明" class="headerlink" title="注意说明"></a>注意说明</h3><p>&emsp;&emsp;通过本次部署大数据平台 Hadoop 的分布式环境，比之前部署伪分布式环境更加熟悉整个过程的搭建，进一步的深刻理解 Liunx 的操作基础以及 Hadoop 的分布式环境搭建，对Hadoop的认识更加的深刻理解。<br>&emsp;&emsp;对 Liunx 的网络配置以及整个集群的静态 IP 设置和网关设置、防火墙的状态查看以及关闭，以及主机名设置和主机名与 IP 进行映射，通过配置 SSH 的非对称加密，通过公钥和私钥实现三台主机之间的相互之间免密登陆。<br>&emsp;&emsp;配置 Hadoop 的核心组件，核心配置文件 core-site.xml，配置 HDFS 的NameNode 地址以及运行时储存目录； HDFS 配置文件 hadoop-env.sh 用于配置 HDFS的 Java 环境， hdfs-site.xml 指定副本数以及辅助名称节点的主机配置； YARN 配置文件 yarn-env.sh 用于配置 YARN 的 Java 环境， yarn-site.xml 配置 YARN 的NodeManger 和 ResourceManger； MapReduce 配置文件 mapred-env.sh 用于配置MapReduce 的 Java 环境， mapred-site.xml 配置 MapReduce 运行在 YARN 上。<br>&emsp;&emsp;采用分发 scp 命令或者采用脚本进行分发集群搭建，以及了解使用 rsync对集群中存在差异的配置文件进行同步更新，以及在集群中常用的时间同步方法以及了解采用部署 ntp 集群实现时间同步。<br>&emsp;&emsp;进一步理解 Hadoop 的各个组件原理，特别是 HDFS 的储存原理，分块储存以及副本机制等等。使用提供的 jar 包，运行 wordcount 程序 jar 包，体验大数据Hadoop 分布式平台以及 MapReduce 数据处理框架。<br>&emsp;&emsp;对于每一个操作步骤不是很清楚的，可以访问本人的GitHub，在学习资料里面有着详细的学习以及安装的过程，是PDF格式的文档说明。</p><h3 id="Hadoop分布式环境搭建详细文档"><a href="#Hadoop分布式环境搭建详细文档" class="headerlink" title="Hadoop分布式环境搭建详细文档"></a><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tLzI2OTQwNDgxNjgvTGVhcm5pbmdNYXRlcmlhbHMvdHJlZS9tYXN0ZXIvJUU1JUE0JUE3JUU2JTk1JUIwJUU2JThEJUFF" title="https://github.com/2694048168/LearningMaterials/tree/master/%E5%A4%A7%E6%95%B0%E6%8D%AE">Hadoop分布式环境搭建详细文档<i class="fa fa-external-link"></i></span></h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;实验目的：&quot;&gt;&lt;a href=&quot;#实验目的：&quot; class=&quot;headerlink&quot; title=&quot;实验目的：&quot;&gt;&lt;/a&gt;实验目的：&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;熟悉 Linux 操作系统的安装以及常用的基本命令&lt;/li&gt;
&lt;li&gt;掌握如何设置静态 IP 地址，掌握
      
    
    </summary>
    
      <category term="Hadoop" scheme="/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="/tags/Hadoop/"/>
    
      <category term="大数据" scheme="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="分布式环境" scheme="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83/"/>
    
  </entry>
  
  <entry>
    <title>Hexo搭建博客</title>
    <link href="/Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/2019/07/18/"/>
    <id>/Hexo搭建博客/2019/07/18/</id>
    <published>2019-07-18T12:06:41.000Z</published>
    <updated>2019-08-11T13:32:38.557Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Hexo简介"><a href="#Hexo简介" class="headerlink" title="Hexo简介"></a>Hexo简介</h2><p>&emsp;&emsp;Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用Markdown（或其他渲染引擎）解析文章，在极短的几秒钟内，就可利用靓丽的主题生成静态网页，特别适合搭建个人博客。Hexo以其简单、高效而且主题丰富多彩而著名，迅速地占据了一部分市场，值得尝试。<br>&emsp;&emsp;本博客就是采用Hexo+GitHubPages+NexT主题进行搭建的！</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv" title="https://hexo.io">Hexo官网<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tLw==" title="https://github.com/">GitHub官网<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0Lm9yZy8=" title="https://theme-next.org/">NexT官网<i class="fa fa-external-link"></i></span></li></ul><h2 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h2><p>&emsp;&emsp;Hexo在搭建时需要Node.js的环境支持，同时需要命令终端的支持。针对不同的操作系统Windows、Linux或者Mac，在配置搭建环境时要下载安装对应的版本，在Windows系统下，建议使用GitBash。Mac和linux都是自带的有BashShell终端。<br>&emsp;&emsp;由于Node.js是国外的网站，有时候由于网络的一些因素，容易影响我们采用npm对资源的访问速度，故此可以采用国内的阿里巴巴的镜像，可以加快访问的速度以及效率。首先在终端采用npm命令安装镜像，成功后就可以采用cnpm命令代替npm命令了。</p><ul><li>npm install -g cnpm –registry=<span class="exturl" data-url="aHR0cHM6Ly9yZWdpc3RyeS5ucG0udGFvYmFvLm9yZw==" title="https://registry.npm.taobao.org">https://registry.npm.taobao.org<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9ub2RlanMub3JnL2VuLw==" title="https://nodejs.org/en/">Node.js<i class="fa fa-external-link"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dheWxhdS9naXQtZm9yLXdpbg==" title="https://github.com/waylau/git-for-win">Git<i class="fa fa-external-link"></i></span></li></ul><h2 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h2><p>&emsp;&emsp;在下载、安装并配置好环境后，就可以进行安装Hexo。首先打开BashShell终端，创建一个空的目录来作为整个博客项目的工作空间，然后再使用命令进行安装hexo，安装成功后就对Hexo进行初始化，初始化成功后可以查看整个工作空间的目录结构和文件树。</p><ul><li>创建一个空文件夹blog：<br>$ mkdir blog<br></li><li>在终端使用命令安装Hexo：<br>$ npm install -g hexo-cli<br></li><li>初始化Hexo：<br>$ hexo init<br></li><li>查看hexo生成的目录:<br>$ cd blog<br>$ ls<br></li></ul><h2 id="配置NexT主题"><a href="#配置NexT主题" class="headerlink" title="配置NexT主题"></a>配置NexT主题</h2><p>&emsp;&emsp;在Hexo的官网可以找到许许多多的主题，寻找适合自己喜欢的主题，可以通过BashShell终端进行下载，也可以下载后解压到站点的主题文件夹下即可，然后打开站点的配置文件，搜索到theme将其值修改为自己下载的主题名即可。</p><ul><li>在终端使用命令下载主题：<br>$ git clone <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RoZW1lLW5leHQvaGV4by10aGVtZS1uZXh0" title="https://github.com/theme-next/hexo-theme-next">https://github.com/theme-next/hexo-theme-next<i class="fa fa-external-link"></i></span> themes/next ./blog/themes/</li><li>修改配置文件_config.yml:<br>$ vim _config.yml<br>将theme的值由landscape修改为hexo-theme-next即可<br></li></ul><h2 id="配置GitHub的Page："><a href="#配置GitHub的Page：" class="headerlink" title="配置GitHub的Page："></a>配置GitHub的Page：</h2><p>&emsp;&emsp;首先要有一个GitHub账号，其次创建一个规定的GitHubPages主页仓库。然后就可以对站点的配置文件进行修改了，添加如下内容即可。而且不仅可以使用GitHub的Pages进行托管，也可以使用国内的Coding的Pages进行托管，同时也可以使用两者进行负载均衡，进行国内外的分流托管。</p><ul><li>修改配置文件_config：<br>在文件末尾的deploy添加内容：<br>deploy:<br>type: git<br>repo: <span class="exturl" data-url="bWFpbHRvOmdpdEBnaXRodWIuY29t" title="mailto:git@github.com">git@github.com<i class="fa fa-external-link"></i></span>:2694048168/2694048168.github.io.git<br>branch: master</li></ul><h2 id="写博客文章"><a href="#写博客文章" class="headerlink" title="写博客文章"></a>写博客文章</h2><p>&emsp;&emsp;编写自己的博客文章(采用markdown格式)，可以在终端采用Vim进行编写，也可以用Notepad++类似的编辑器编写，只要博客文章所在的路径是正确的，就可以被Hexo识别并读取到即可。</p><ul><li>在终端命令创建文章：<br>$ hexo new file_name <br></li><li>使用Vim等编辑器编辑文章</li></ul><h2 id="更新Hexo并上传"><a href="#更新Hexo并上传" class="headerlink" title="更新Hexo并上传"></a>更新Hexo并上传</h2><p>&emsp;&emsp;在更新上传之前可以在本地进行预览一下，即就是先清除clean、生成generate、然后start启动本地，在本地的4000端口查看即可。没有问题后，在进行上传deploy。当然这些常用的命令都是可以编写一个shell脚本进行的，因为每次都需要的，强烈建议写成一个shell脚本。</p><ul><li>在终端依次使用命令：<br>$ hexo clean<br>$ hexo g<br>$ hexo d <br></li></ul><h2 id="终端访问"><a href="#终端访问" class="headerlink" title="终端访问"></a>终端访问</h2><p>&emsp;&emsp;通过PC端浏览器或者智能终端浏览器访问即可。</p><ul><li>本地访问：<br><span class="exturl" data-url="aHR0cDovL2xvY2FsaG9zdDo0MDAw" title="http://localhost:4000">http://localhost:4000<i class="fa fa-external-link"></i></span></li><li>访问地址：<br><span class="exturl" data-url="aHR0cHM6Ly8yNjk0MDQ4MTY4LmdpdGh1Yi5pbw==" title="https://2694048168.github.io">https://2694048168.github.io<i class="fa fa-external-link"></i></span></li></ul><h2 id="注意说明"><a href="#注意说明" class="headerlink" title="注意说明"></a>注意说明</h2><ul><li>在Windows系统下，终端采用GitBash即可</li><li>在Linux系统下，终端采用自带的Bash即可</li><li>Linux系统用户需要注意命令的权限问题</li><li>在Mac系统下，终端采用自带的Bash即可</li><li>整个操作过程全部都在blog目录下，注意操作命令的路径问题</li><li>博客文章格式采用Markdown</li><li>hexo s 命令是启动本地hexo，访问通过<span class="exturl" data-url="aHR0cDovL2xvY2FsaG9zdDo0MDAw" title="http://localhost:4000">http://localhost:4000<i class="fa fa-external-link"></i></span></li><li>关于github的page详情查看：<span class="exturl" data-url="aHR0cHM6Ly9wYWdlcy5naXRodWIuY29tLw==" title="https://pages.github.com/">github.pages<i class="fa fa-external-link"></i></span></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Hexo简介&quot;&gt;&lt;a href=&quot;#Hexo简介&quot; class=&quot;headerlink&quot; title=&quot;Hexo简介&quot;&gt;&lt;/a&gt;Hexo简介&lt;/h2&gt;&lt;p&gt;&amp;emsp;&amp;emsp;Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用Markdown（或其他渲
      
    
    </summary>
    
      <category term="工具" scheme="/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Hexo" scheme="/tags/Hexo/"/>
    
      <category term="GitHub" scheme="/tags/GitHub/"/>
    
      <category term="NexT" scheme="/tags/NexT/"/>
    
  </entry>
  
</feed>
